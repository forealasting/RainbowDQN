{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !apt install python-opengl\n",
    "    !apt install ffmpeg\n",
    "    !apt install xvfb\n",
    "    !pip install pyvirtualdisplay\n",
    "    !pip install gym\n",
    "    from pyvirtualdisplay import Display\n",
    "    \n",
    "    # Start virtual display\n",
    "    dis = Display(visible=0, size=(400, 400))\n",
    "    dis.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Categorical DQN\n",
    "\n",
    "[M. G. Bellemare et al., \"A Distributional Perspective on Reinforcement Learning.\" arXiv preprint arXiv:1707.06887, 2017.](https://arxiv.org/pdf/1707.06887.pdf)\n",
    "\n",
    "The authors argued the importance of learning the distribution of returns instead of the expected return, and they proposed to model such distributions with probability masses placed on a discrete support $z$, where $z$ is a vector with $N_{atoms} \\in \\mathbb{N}^+$ atoms, defined by $z_i = V_{min} + (i-1) \\frac{V_{max} - V_{min}}{N-1}$ for $i \\in \\{1, ..., N_{atoms}\\}$.\n",
    "\n",
    "The key insight is that return distributions satisfy a variant of Bellmanâ€™s equation. For a given state $S_t$ and action $A_t$, the distribution of the returns under the optimal policy $\\pi^{*}$ should match a target distribution defined by taking the distribution for the next state $S_{t+1}$ and action $a^{*}_{t+1} = \\pi^{*}(S_{t+1})$, contracting\n",
    "it towards zero according to the discount, and shifting it by the reward (or distribution of rewards, in the stochastic case). A distributional variant of Q-learning is then derived by first constructing a new support for the target distribution, and then minimizing the Kullbeck-Leibler divergence between the distribution $d_t$ and the target distribution\n",
    "\n",
    "$$\n",
    "d_t' = (R_{t+1} + \\gamma_{t+1} z, p_\\hat{{\\theta}} (S_{t+1}, \\hat{a}^{*}_{t+1})),\\\\\n",
    "D_{KL} (\\phi_z d_t' \\| d_t).\n",
    "$$\n",
    "\n",
    "Here $\\phi_z$ is a L2-projection of the target distribution onto the fixed support $z$, and $\\hat{a}^*_{t+1} = \\arg\\max_{a} q_{\\hat{\\theta}} (S_{t+1}, a)$ is the greedy action with respect to the mean action values $q_{\\hat{\\theta}} (S_{t+1}, a) = z^{T}p_{\\theta}(S_{t+1}, a)$ in state $S_{t+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gwhsu\\anaconda3\\envs\\rainbow-is-all-you-need\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# if you use another  jupytertheme\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Please see *01.dqn.ipynb* for detailed description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "\n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "The parametrized distribution can be represented by a neural network, as in DQN, but with atom_size x out_dim outputs. A softmax is applied independently for each action dimension of the output to ensure that the distribution for each action is appropriately normalized.\n",
    "\n",
    "To estimate q-values, we use inner product of each action's softmax distribution and support which is the set of atoms $\\{z_i = V_{min} + i\\Delta z: 0 \\le i < N\\}, \\Delta z = \\frac{V_{max} - V_{min}}{N-1}$.\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = \\sum_i z_i p_i(s_t, a_t), \\\\\n",
    "\\text{where } p_i \\text{ is the probability of } z_i \\text{ (the output of softmax)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int, \n",
    "        atom_size: int, \n",
    "        support: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.support = support\n",
    "        self.out_dim = out_dim\n",
    "        self.atom_size = atom_size\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_dim * atom_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        dist = self.dist(x) \n",
    "        # print(dist) # [32, 2, 51]\n",
    "        \n",
    "        q = torch.sum(dist * self.support, dim=2)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def dist(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get distribution for atoms.\"\"\"\n",
    "        q_atoms = self.layers(x).view(-1, self.out_dim, self.atom_size)\n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "        dist = dist.clamp(min=1e-3)  # for avoiding nans\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical DQN Agent\n",
    "\n",
    "\n",
    "Here is a summary of DQNAgent class.\n",
    "\n",
    "| Method           | Note                                                 |\n",
    "| ---              | ---                                                  |\n",
    "|select_action     | select an action from the input state.               |\n",
    "|step              | take an action and return the response of the env.   |\n",
    "|compute_dqn_loss  | return dqn loss.                                     |\n",
    "|update_model      | update the model by gradient descent.                |\n",
    "|target_hard_update| hard update from the local model to the target model.|\n",
    "|train             | train the agent during num_frames.                   |\n",
    "|test              | test the agent (1 episode).                          |\n",
    "|plot              | plot the training progresses.                        |\n",
    "\n",
    "All differences from pure DQN are noted with comments *Categorical DQN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        memory (ReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        epsilon (float): parameter for epsilon greedy policy\n",
    "        epsilon_decay (float): step size to decrease epsilon\n",
    "        max_epsilon (float): max value of epsilon\n",
    "        min_epsilon (float): min value of epsilon\n",
    "        target_update (int): period for target model's hard update\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including\n",
    "                           state, action, reward, next_state, done\n",
    "        v_min (float): min value of support\n",
    "        v_max (float): max value of support\n",
    "        atom_size (int): the unit number of support\n",
    "        support (torch.Tensor): support for categorical dqn\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        epsilon_decay: float,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.1,\n",
    "        gamma: float = 0.99,\n",
    "        # Categorical DQN parameters\n",
    "        v_min: float = 0.0,\n",
    "        v_max: float = 200.0,\n",
    "        atom_size: int = 51,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            epsilon_decay (float): step size to decrease epsilon\n",
    "            lr (float): learning rate\n",
    "            max_epsilon (float): max value of epsilon\n",
    "            min_epsilon (float): min value of epsilon\n",
    "            gamma (float): discount factor\n",
    "            v_min (float): min value of support\n",
    "            v_max (float): max value of support\n",
    "            atom_size (int): the unit number of support\n",
    "        \"\"\"\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = max_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        \n",
    "        # Categorical DQN parameters\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atom_size = atom_size\n",
    "        self.support = torch.linspace(\n",
    "            self.v_min, self.v_max, self.atom_size\n",
    "        ).to(self.device)\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(\n",
    "            obs_dim, action_dim, atom_size, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target = Network(\n",
    "            obs_dim, action_dim, atom_size, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # epsilon greedy policy\n",
    "        if self.epsilon > np.random.random():\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            selected_action = self.dqn(\n",
    "                torch.FloatTensor(state).to(self.device),\n",
    "            ).argmax()\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        \n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "    \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        samples = self.memory.sample_batch()\n",
    "\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        update_cnt = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "        mov_avg = []\n",
    "\n",
    "        for frame_idx in range(1, num_frames + 1):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                scores.append(score)\n",
    "                mov_avg.append(np.mean(scores[-5:]))\n",
    "                score = 0\n",
    "\n",
    "            # if training is ready\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "                \n",
    "                # linearly decrease epsilon\n",
    "                self.epsilon = max(\n",
    "                    self.min_epsilon, self.epsilon - (\n",
    "                        self.max_epsilon - self.min_epsilon\n",
    "                    ) * self.epsilon_decay\n",
    "                )\n",
    "                epsilons.append(self.epsilon)\n",
    "                \n",
    "                # if hard update is needed\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "\n",
    "            # plotting\n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                # self._plot(frame_idx, scores, losses, epsilons, mov_avg)\n",
    "                continue\n",
    "                \n",
    "        self.env.close()\n",
    "                \n",
    "    def test(self) -> List[np.ndarray]:\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "        \n",
    "        return frames\n",
    "\n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"Return categorical dqn loss.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "        \n",
    "        # Categorical DQN algorithm\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action = self.dqn_target(next_state).argmax(1)\n",
    "            next_dist = self.dqn_target.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * self.gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n",
    "                ).long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.atom_size)\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn.dist(state)\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "\n",
    "        loss = -(proj_dist * log_p).sum(1).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "                \n",
    "    def _plot(\n",
    "        self, \n",
    "        frame_idx: int, \n",
    "        scores: List[float], \n",
    "        losses: List[float], \n",
    "        epsilons: List[float],\n",
    "        mov_avg : List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-5:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        # moving avg\n",
    "        plt.subplot(133)\n",
    "        plt.title('mov_avg')\n",
    "        plt.plot(mov_avg)\n",
    "        # plt.subplot(133)\n",
    "        # plt.title('epsilons')\n",
    "        # plt.plot(epsilons)\n",
    "        if frame_idx == 20000:\n",
    "\n",
    "            loc_dt = datetime.datetime.today() \n",
    "            loc_dt = str(loc_dt)\n",
    "            name = loc_dt.split(' ')[1]\n",
    "            name_ = name.split(':')[0] + '_' + name.split(':')[1] + '.png'\n",
    "            img_name = 'Categorical_DQN/' + str(name_) \n",
    "            # plt.savefig(img_name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "You can see the [code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and [configurations](https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L53) of CartPole-v0 from OpenAI's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)\n",
    "if IN_COLAB:\n",
    "    env = gym.wrappers.Monitor(env, \"videos\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[777]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 777\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "num_frames = 20000\n",
    "memory_size = 2000\n",
    "batch_size = 128\n",
    "target_update = 200\n",
    "epsilon_decay = 1 / 2000\n",
    "\n",
    "# train\n",
    "agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([1, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([1, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([1, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([1, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n",
      "torch.Size([32, 2, 51])\n"
     ]
    }
   ],
   "source": [
    "agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(10):\n",
    "    agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay)\n",
    "    agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Run the trained agent (1 episode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frames = agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if IN_COLAB:  # for colab\n",
    "    import base64\n",
    "    import glob\n",
    "    import io\n",
    "    import os\n",
    "\n",
    "    from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "    def ipython_show_video(path: str) -> None:\n",
    "        \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
    "        if not os.path.isfile(path):\n",
    "            raise NameError(\"Cannot access: {}\".format(path))\n",
    "\n",
    "        video = io.open(path, \"r+b\").read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        display(HTML(\n",
    "            data=\"\"\"\n",
    "            <video alt=\"test\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
    "            </video>\n",
    "            \"\"\".format(encoded.decode(\"ascii\"))\n",
    "        ))\n",
    "\n",
    "    list_of_files = glob.glob(\"videos/*.mp4\")\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(latest_file)\n",
    "    ipython_show_video(latest_file)\n",
    "    \n",
    "else:  # for jupyter\n",
    "    from matplotlib import animation\n",
    "    from JSAnimation.IPython_display import display_animation\n",
    "    from IPython.display import display\n",
    "\n",
    "\n",
    "    def display_frames_as_gif(frames: List[np.ndarray]) -> None:\n",
    "        \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
    "        patch = plt.imshow(frames[0])\n",
    "        plt.axis('off')\n",
    "\n",
    "        def animate(i):\n",
    "            patch.set_data(frames[i])\n",
    "\n",
    "        anim = animation.FuncAnimation(\n",
    "            plt.gcf(), animate, frames = len(frames), interval=50\n",
    "        )\n",
    "        display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "\n",
    "    # display \n",
    "    display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def save_frames_as_gif(frames, path='./', filename='gif/Categorical_DQN.gif'):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Rainbow', fontsize=14)\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    \n",
    "    \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "save_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
